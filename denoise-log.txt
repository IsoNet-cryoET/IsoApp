Command: isonet.py denoise --star_file tomograms.star --output_dir denoise --gpuID 1,2 --ncpus 16 --arch unet-medium --pretrained_model None --pretrained_model2 None --cube_size 96 --epochs 4 --input_column rlnTomoReconstructedTomogramHalf --batch_size None --acc_batches 1 --loss_func L2 --learning_rate 0.0003 --T_max 2 --learning_rate_min 0.0003 --compile_model False --mixed_precision True --CTF_mode None --isCTFflipped False --with_predict True --split_halves False --snrfalloff 0 --deconvstrength 1 --highpassnyquist 0.02
07-18 19:08:29, INFO     Port number: 59847
Total number of parameters: 23989825
Preprocess tomograms:   0%|                                                   | 0/5 [00:00<?, ?it/s]Preprocess tomograms:  20%|████████▌                                  | 1/5 [00:00<00:01,  2.34it/s]Preprocess tomograms:  40%|█████████████████▏                         | 2/5 [00:00<00:01,  2.52it/s]Preprocess tomograms:  60%|█████████████████████████▊                 | 3/5 [00:01<00:00,  2.58it/s]Preprocess tomograms:  80%|██████████████████████████████████▍        | 4/5 [00:01<00:00,  2.60it/s]Preprocess tomograms: 100%|███████████████████████████████████████████| 5/5 [00:01<00:00,  2.57it/s]Preprocess tomograms: 100%|███████████████████████████████████████████| 5/5 [00:01<00:00,  2.56it/s]
training tomograms for 2 epochs, remaining epochs 4
Epoch 1:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 1:   0%| | 0/13 [00:19<?, ?batch/s, Loss: 1.6821 | in_mw_loss: 1.6821 | oEpoch 1:   8%| | 1/13 [00:19<03:50, 19.23s/batch, Loss: 1.6821 | in_mw_loss: 1.Epoch 1:   8%| | 1/13 [00:19<03:50, 19.23s/batch, Loss: 1.6087 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:28,  8.02s/batch, Loss: 1.6087 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:28,  8.02s/batch, Loss: 1.3491 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.42s/batch, Loss: 1.3491 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.42s/batch, Loss: 1.2650 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:19<00:24,  2.74s/batch, Loss: 1.2650 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:24,  2.74s/batch, Loss: 1.2480 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.87s/batch, Loss: 1.2480 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.87s/batch, Loss: 1.1623 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:08,  1.28s/batch, Loss: 1.1623 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:08,  1.28s/batch, Loss: 1.1673 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.09batch/s, Loss: 1.1673 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.09batch/s, Loss: 1.1890 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.49batch/s, Loss: 1.1890 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.49batch/s, Loss: 1.1462 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:20<00:02,  1.99batch/s, Loss: 1.1462 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:20<00:02,  1.99batch/s, Loss: 1.1352 | in_mw_loss: 1.Epoch 1:  77%|▊| 10/13 [00:20<00:01,  2.35batch/s, Loss: 1.1352 | in_mw_loss: 1Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.35batch/s, Loss: 1.1548 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.69batch/s, Loss: 1.1548 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.69batch/s, Loss: 1.0121 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.26batch/s, Loss: 1.0121 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.26batch/s, Loss: 1.0873 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  3.98batch/s, Loss: 1.0873 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  1.65s/batch, Loss: 1.0873 | in_mw_loss: 1
Epoch [  1/  2], Loss: 1.2576, in_mw_loss: 1.2576, out_mw_loss: 1.2576, learning_rate: 3.0000e-04
Epoch 2:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 2:   0%| | 0/13 [00:00<?, ?batch/s, Loss: 1.1227 | in_mw_loss: 1.1227 | oEpoch 2:   8%| | 1/13 [00:00<00:04,  2.95batch/s, Loss: 1.1227 | in_mw_loss: 1.Epoch 2:   8%| | 1/13 [00:00<00:04,  2.95batch/s, Loss: 1.0188 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  4.50batch/s, Loss: 1.0188 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  4.50batch/s, Loss: 1.1824 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.29batch/s, Loss: 1.1824 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.29batch/s, Loss: 1.0552 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  5.76batch/s, Loss: 1.0552 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:01<00:01,  5.76batch/s, Loss: 1.0461 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.71batch/s, Loss: 1.0461 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.71batch/s, Loss: 1.1032 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.66batch/s, Loss: 1.1032 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.66batch/s, Loss: 1.0704 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  5.21batch/s, Loss: 1.0704 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  5.21batch/s, Loss: 1.0633 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  5.63batch/s, Loss: 1.0633 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  5.63batch/s, Loss: 0.9925 | in_mw_loss: 0.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  6.11batch/s, Loss: 0.9925 | in_mw_loss: 0.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  6.11batch/s, Loss: 1.0659 | in_mw_loss: 1.Epoch 2:  77%|▊| 10/13 [00:01<00:00,  4.91batch/s, Loss: 1.0659 | in_mw_loss: 1Epoch 2:  77%|▊| 10/13 [00:02<00:00,  4.91batch/s, Loss: 1.0431 | in_mw_loss: 1Epoch 2:  85%|▊| 11/13 [00:02<00:00,  5.00batch/s, Loss: 1.0431 | in_mw_loss: 1Epoch 2:  85%|▊| 11/13 [00:02<00:00,  5.00batch/s, Loss: 1.1292 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  5.33batch/s, Loss: 1.1292 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  5.33batch/s, Loss: 1.0162 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:02<00:00,  5.33batch/s, Loss: 1.0162 | in_mw_loss: 1
Epoch [  2/  2], Loss: 1.0534, in_mw_loss: 1.0534, out_mw_loss: 1.0534, learning_rate: 3.0000e-04
data_shape torch.Size([320, 1, 96, 96, 96])
predict:   0%|                                         | 0/160 [00:00<?, ?it/s]predict:   1%|▏                                | 1/160 [00:00<00:27,  5.78it/s]predict:   3%|█                                | 5/160 [00:00<00:08, 19.09it/s]predict:   6%|█▊                               | 9/160 [00:00<00:06, 24.86it/s]predict:   8%|██▍                             | 12/160 [00:00<00:05, 25.32it/s]predict:   9%|███                             | 15/160 [00:00<00:06, 22.23it/s]predict:  11%|███▌                            | 18/160 [00:00<00:06, 20.67it/s]predict:  13%|████▏                           | 21/160 [00:00<00:06, 22.40it/s]predict:  16%|█████                           | 25/160 [00:01<00:05, 26.56it/s]predict:  18%|█████▊                          | 29/160 [00:01<00:04, 29.51it/s]predict:  21%|██████▌                         | 33/160 [00:01<00:04, 31.62it/s]predict:  23%|███████▍                        | 37/160 [00:01<00:04, 30.05it/s]predict:  26%|████████▏                       | 41/160 [00:01<00:04, 24.39it/s]predict:  28%|█████████                       | 45/160 [00:01<00:04, 26.96it/s]predict:  30%|█████████▌                      | 48/160 [00:01<00:04, 26.23it/s]predict:  32%|██████████▍                     | 52/160 [00:02<00:03, 28.48it/s]predict:  35%|███████████▏                    | 56/160 [00:02<00:03, 27.71it/s]predict:  37%|███████████▊                    | 59/160 [00:02<00:04, 24.18it/s]predict:  39%|████████████▍                   | 62/160 [00:02<00:04, 21.93it/s]predict:  41%|█████████████▏                  | 66/160 [00:02<00:03, 24.74it/s]predict:  44%|██████████████                  | 70/160 [00:02<00:03, 27.77it/s]predict:  46%|██████████████▊                 | 74/160 [00:02<00:02, 30.24it/s]predict:  49%|███████████████▌                | 78/160 [00:02<00:02, 31.68it/s]predict:  51%|████████████████▍               | 82/160 [00:03<00:02, 27.45it/s]predict:  53%|█████████████████               | 85/160 [00:03<00:03, 24.00it/s]predict:  56%|█████████████████▊              | 89/160 [00:03<00:02, 26.83it/s]predict:  57%|██████████████████▍             | 92/160 [00:03<00:02, 26.08it/s]predict:  60%|███████████████████▏            | 96/160 [00:03<00:02, 28.34it/s]predict:  62%|███████████████████▍           | 100/160 [00:03<00:02, 29.49it/s]predict:  65%|████████████████████▏          | 104/160 [00:04<00:02, 24.47it/s]predict:  67%|████████████████████▋          | 107/160 [00:04<00:02, 22.35it/s]predict:  69%|█████████████████████▎         | 110/160 [00:04<00:02, 23.46it/s]predict:  71%|██████████████████████         | 114/160 [00:04<00:01, 26.68it/s]predict:  74%|██████████████████████▊        | 118/160 [00:04<00:01, 29.44it/s]predict:  76%|███████████████████████▋       | 122/160 [00:04<00:01, 31.53it/s]predict:  79%|████████████████████████▍      | 126/160 [00:04<00:01, 29.86it/s]predict:  81%|█████████████████████████▏     | 130/160 [00:05<00:01, 24.38it/s]predict:  84%|█████████████████████████▉     | 134/160 [00:05<00:00, 27.14it/s]predict:  86%|██████████████████████████▌    | 137/160 [00:05<00:00, 26.23it/s]predict:  88%|███████████████████████████▎   | 141/160 [00:05<00:00, 28.53it/s]predict:  91%|████████████████████████████   | 145/160 [00:05<00:00, 30.08it/s]predict:  93%|████████████████████████████▊  | 149/160 [00:05<00:00, 25.98it/s]predict:  95%|█████████████████████████████▍ | 152/160 [00:05<00:00, 22.98it/s]predict:  97%|██████████████████████████████ | 155/160 [00:05<00:00, 23.58it/s]predict:  99%|██████████████████████████████▊| 159/160 [00:06<00:00, 26.49it/s]predict: 100%|███████████████████████████████| 160/160 [00:06<00:00, 26.12it/s]
size restored (330, 522, 522)
Saved all slices and square power spectrum for epoch 2 to 'denoise'
training tomograms for 2 epochs, remaining epochs 2
Epoch 1:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 1:   0%| | 0/13 [00:19<?, ?batch/s, Loss: 1.6830 | in_mw_loss: 1.6830 | oEpoch 1:   8%| | 1/13 [00:19<03:53, 19.45s/batch, Loss: 1.6830 | in_mw_loss: 1.Epoch 1:   8%| | 1/13 [00:19<03:53, 19.45s/batch, Loss: 1.6075 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:29,  8.11s/batch, Loss: 1.6075 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:29,  8.11s/batch, Loss: 1.3491 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:45,  4.55s/batch, Loss: 1.3491 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:20<00:45,  4.55s/batch, Loss: 1.2651 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.81s/batch, Loss: 1.2651 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.81s/batch, Loss: 1.2479 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.85s/batch, Loss: 1.2479 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.85s/batch, Loss: 1.1626 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:08,  1.27s/batch, Loss: 1.1626 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:08,  1.27s/batch, Loss: 1.1682 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.03batch/s, Loss: 1.1682 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.03batch/s, Loss: 1.1893 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.40batch/s, Loss: 1.1893 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:21<00:03,  1.40batch/s, Loss: 1.1464 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:21<00:02,  1.87batch/s, Loss: 1.1464 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:21<00:02,  1.87batch/s, Loss: 1.1362 | in_mw_loss: 1.Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.08batch/s, Loss: 1.1362 | in_mw_loss: 1Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.08batch/s, Loss: 1.1554 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.66batch/s, Loss: 1.1554 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.66batch/s, Loss: 1.0168 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.23batch/s, Loss: 1.0168 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.23batch/s, Loss: 1.0847 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  3.67batch/s, Loss: 1.0847 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  1.68s/batch, Loss: 1.0847 | in_mw_loss: 1
Epoch [  1/  2], Loss: 1.2581, in_mw_loss: 1.2581, out_mw_loss: 1.2581, learning_rate: 3.0000e-04
Epoch 2:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 2:   0%| | 0/13 [00:00<?, ?batch/s, Loss: 1.1247 | in_mw_loss: 1.1247 | oEpoch 2:   8%| | 1/13 [00:00<00:03,  3.40batch/s, Loss: 1.1247 | in_mw_loss: 1.Epoch 2:   8%| | 1/13 [00:00<00:03,  3.40batch/s, Loss: 1.0183 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  4.99batch/s, Loss: 1.0183 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  4.99batch/s, Loss: 1.1792 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.45batch/s, Loss: 1.1792 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.45batch/s, Loss: 1.0534 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  6.10batch/s, Loss: 1.0534 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  6.10batch/s, Loss: 1.0453 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:00<00:01,  6.51batch/s, Loss: 1.0453 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:00<00:01,  6.51batch/s, Loss: 1.1047 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:00<00:01,  6.79batch/s, Loss: 1.1047 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  6.79batch/s, Loss: 1.0704 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:00,  6.98batch/s, Loss: 1.0704 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:00,  6.98batch/s, Loss: 1.0637 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  7.11batch/s, Loss: 1.0637 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  7.11batch/s, Loss: 0.9928 | in_mw_loss: 0.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  7.21batch/s, Loss: 0.9928 | in_mw_loss: 0.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  7.21batch/s, Loss: 1.0659 | in_mw_loss: 1.Epoch 2:  77%|▊| 10/13 [00:01<00:00,  7.26batch/s, Loss: 1.0659 | in_mw_loss: 1Epoch 2:  77%|▊| 10/13 [00:01<00:00,  7.26batch/s, Loss: 1.0434 | in_mw_loss: 1Epoch 2:  85%|▊| 11/13 [00:01<00:00,  7.33batch/s, Loss: 1.0434 | in_mw_loss: 1Epoch 2:  85%|▊| 11/13 [00:01<00:00,  7.33batch/s, Loss: 1.1266 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:01<00:00,  7.33batch/s, Loss: 1.1266 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:01<00:00,  7.33batch/s, Loss: 1.0162 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:01<00:00,  6.83batch/s, Loss: 1.0162 | in_mw_loss: 1
Epoch [  2/  2], Loss: 1.0528, in_mw_loss: 1.0528, out_mw_loss: 1.0528, learning_rate: 3.0000e-04
data_shape torch.Size([320, 1, 96, 96, 96])
predict:   0%|                                         | 0/160 [00:00<?, ?it/s]predict:   1%|▏                                | 1/160 [00:00<00:25,  6.12it/s]predict:   2%|▊                                | 4/160 [00:00<00:10, 14.45it/s]predict:   4%|█▏                               | 6/160 [00:00<00:09, 15.89it/s]predict:   5%|█▋                               | 8/160 [00:00<00:09, 16.77it/s]predict:   7%|██▏                             | 11/160 [00:00<00:07, 20.11it/s]predict:   9%|███                             | 15/160 [00:00<00:05, 25.81it/s]predict:  12%|███▊                            | 19/160 [00:00<00:05, 27.65it/s]predict:  14%|████▌                           | 23/160 [00:00<00:04, 30.20it/s]predict:  17%|█████▍                          | 27/160 [00:01<00:04, 32.10it/s]predict:  19%|██████▏                         | 31/160 [00:01<00:04, 27.85it/s]predict:  21%|██████▊                         | 34/160 [00:01<00:04, 26.86it/s]predict:  23%|███████▍                        | 37/160 [00:01<00:05, 23.93it/s]predict:  26%|████████▏                       | 41/160 [00:01<00:04, 27.40it/s]predict:  28%|█████████                       | 45/160 [00:01<00:03, 30.27it/s]predict:  31%|█████████▊                      | 49/160 [00:01<00:03, 30.84it/s]predict:  33%|██████████▌                     | 53/160 [00:02<00:03, 32.15it/s]predict:  36%|███████████▍                    | 57/160 [00:02<00:03, 33.67it/s]predict:  38%|████████████▏                   | 61/160 [00:02<00:03, 30.07it/s]predict:  41%|█████████████                   | 65/160 [00:02<00:03, 24.94it/s]predict:  42%|█████████████▌                  | 68/160 [00:02<00:03, 25.61it/s]predict:  45%|██████████████▍                 | 72/160 [00:02<00:03, 28.44it/s]predict:  48%|███████████████▏                | 76/160 [00:02<00:02, 30.10it/s]predict:  50%|████████████████                | 80/160 [00:02<00:02, 31.44it/s]predict:  52%|████████████████▊               | 84/160 [00:03<00:02, 32.42it/s]predict:  55%|█████████████████▌              | 88/160 [00:03<00:02, 31.39it/s]predict:  57%|██████████████████▍             | 92/160 [00:03<00:02, 25.91it/s]predict:  59%|███████████████████             | 95/160 [00:03<00:02, 24.30it/s]predict:  62%|███████████████████▊            | 99/160 [00:03<00:02, 27.58it/s]predict:  64%|███████████████████▉           | 103/160 [00:03<00:01, 29.42it/s]predict:  67%|████████████████████▋          | 107/160 [00:03<00:01, 30.74it/s]predict:  69%|█████████████████████▌         | 111/160 [00:04<00:01, 32.28it/s]predict:  72%|██████████████████████▎        | 115/160 [00:04<00:01, 30.28it/s]predict:  74%|███████████████████████        | 119/160 [00:04<00:01, 25.39it/s]predict:  76%|███████████████████████▋       | 122/160 [00:04<00:01, 24.74it/s]predict:  79%|████████████████████████▍      | 126/160 [00:04<00:01, 27.88it/s]predict:  81%|█████████████████████████▏     | 130/160 [00:04<00:00, 30.49it/s]predict:  84%|█████████████████████████▉     | 134/160 [00:04<00:00, 30.58it/s]predict:  86%|██████████████████████████▋    | 138/160 [00:04<00:00, 32.42it/s]predict:  89%|███████████████████████████▌   | 142/160 [00:05<00:00, 33.83it/s]predict:  91%|████████████████████████████▎  | 146/160 [00:05<00:00, 32.39it/s]Traceback (most recent call last):
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 1080, in <module>
    exit(main())
         ~~~~^^
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 1076, in main
    fire.Fire(ISONET)
    ~~~~~~~~~^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ~~~~~~~~~~~~~~~~~~~^
        component,
        ^^^^^^^^^^
    ...<2 lines>...
        treatment='class' if is_class else 'routine',
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        target=component.__name__)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 604, in denoise
    network.train(training_params) #train based on init model and save new one as model_iter{num_iter}.h5
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 257, in train
    tmp_out = self.predict_map(tomo_vol,output_dir=f"{training_params['output_dir']}", F_mask=F_mask).astype(np.float32) * -1
              ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 315, in predict_map
    outData = self.predict(data, tmp_data_path=tmp_data_path, F_mask=F_mask)
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 294, in predict
    mp.spawn(ddp_predict, args=(self.world_size, self.port_number, self.model, data, tmp_data_path,\
    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                 F_mask), nprocs=self.world_size)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
              ~~~~~~~~~~~~^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 144, in join
    ready = multiprocessing.connection.wait(
        self.sentinels.keys(),
        timeout=timeout,
    )
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/multiprocessing/connection.py", line 1148, in wait
    ready = selector.select(timeout)
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/selectors.py", line 398, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
predict:  93%|████████████████████████████▊  | 149/160 [00:05<00:00, 27.51it/s]
[rank0]:[W718 19:10:14.974669767 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Exception ignored in atexit callback <function _exit_function at 0x7efd5344de40>:
Traceback (most recent call last):
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/multiprocessing/util.py", line 349, in _exit_function
    p.join()
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/multiprocessing/popen_fork.py", line 44, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt: 
Command: isonet.py denoise --star_file tomograms.star --output_dir denoise --gpuID 0,1 --ncpus 16 --arch unet-medium --pretrained_model None --pretrained_model2 None --cube_size 96 --epochs 6 --input_column rlnTomoReconstructedTomogramHalf --batch_size None --acc_batches 1 --loss_func L2 --learning_rate 0.0003 --T_max 2 --learning_rate_min 0.0003 --compile_model False --mixed_precision True --CTF_mode None --isCTFflipped False --with_predict True --split_halves False --snrfalloff 0 --deconvstrength 1 --highpassnyquist 0.02
07-18 20:11:18, INFO     The denoise folder already exists, outputs will write into this folder
07-18 20:11:18, INFO     Port number: 52099
Total number of parameters: 23989825
Preprocess tomograms:   0%|                                                   | 0/5 [00:00<?, ?it/s]Preprocess tomograms:  20%|████████▌                                  | 1/5 [00:00<00:01,  2.65it/s]Preprocess tomograms:  40%|█████████████████▏                         | 2/5 [00:00<00:01,  2.68it/s]Preprocess tomograms:  60%|█████████████████████████▊                 | 3/5 [00:01<00:00,  2.68it/s]Preprocess tomograms:  80%|██████████████████████████████████▍        | 4/5 [00:01<00:00,  2.68it/s]Preprocess tomograms: 100%|███████████████████████████████████████████| 5/5 [00:01<00:00,  2.71it/s]Preprocess tomograms: 100%|███████████████████████████████████████████| 5/5 [00:01<00:00,  2.69it/s]
training tomograms for 2 epochs, remaining epochs 6
Epoch 1:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 1:   0%| | 0/13 [00:19<?, ?batch/s, Loss: 1.7158 | in_mw_loss: 1.7158 | oEpoch 1:   8%| | 1/13 [00:19<03:52, 19.38s/batch, Loss: 1.7158 | in_mw_loss: 1.Epoch 1:   8%| | 1/13 [00:19<03:52, 19.38s/batch, Loss: 1.5928 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:29,  8.17s/batch, Loss: 1.5928 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:29,  8.17s/batch, Loss: 1.3917 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:45,  4.51s/batch, Loss: 1.3917 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:20<00:45,  4.51s/batch, Loss: 1.3107 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.80s/batch, Loss: 1.3107 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.80s/batch, Loss: 1.1391 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.85s/batch, Loss: 1.1391 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.85s/batch, Loss: 1.2693 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.32s/batch, Loss: 1.2693 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.32s/batch, Loss: 1.1310 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.05batch/s, Loss: 1.1310 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.05batch/s, Loss: 1.1562 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.39batch/s, Loss: 1.1562 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:21<00:03,  1.39batch/s, Loss: 1.1498 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:21<00:02,  1.75batch/s, Loss: 1.1498 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:21<00:02,  1.75batch/s, Loss: 1.1429 | in_mw_loss: 1.Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.23batch/s, Loss: 1.1429 | in_mw_loss: 1Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.23batch/s, Loss: 1.1406 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.54batch/s, Loss: 1.1406 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.54batch/s, Loss: 1.1562 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  2.93batch/s, Loss: 1.1562 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  2.93batch/s, Loss: 1.1159 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  3.52batch/s, Loss: 1.1159 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  1.69s/batch, Loss: 1.1159 | in_mw_loss: 1
Epoch [  1/  2], Loss: 1.2667, in_mw_loss: 1.2667, out_mw_loss: 1.2667, learning_rate: 3.0000e-04
Epoch 2:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 2:   0%| | 0/13 [00:00<?, ?batch/s, Loss: 1.1081 | in_mw_loss: 1.1081 | oEpoch 2:   8%| | 1/13 [00:00<00:02,  4.32batch/s, Loss: 1.1081 | in_mw_loss: 1.Epoch 2:   8%| | 1/13 [00:00<00:02,  4.32batch/s, Loss: 1.1153 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  5.01batch/s, Loss: 1.1153 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  5.01batch/s, Loss: 1.0760 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:02,  4.28batch/s, Loss: 1.0760 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:02,  4.28batch/s, Loss: 1.1439 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:02,  4.44batch/s, Loss: 1.1439 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:01<00:02,  4.44batch/s, Loss: 1.0597 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.71batch/s, Loss: 1.0597 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.71batch/s, Loss: 1.0445 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.20batch/s, Loss: 1.0445 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.20batch/s, Loss: 1.0248 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  4.42batch/s, Loss: 1.0248 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  4.42batch/s, Loss: 1.0420 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:01,  4.72batch/s, Loss: 1.0420 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:02<00:01,  4.72batch/s, Loss: 1.0674 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:02<00:00,  4.34batch/s, Loss: 1.0674 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:02<00:00,  4.34batch/s, Loss: 0.9844 | in_mw_loss: 0.Epoch 2:  77%|▊| 10/13 [00:02<00:00,  4.46batch/s, Loss: 0.9844 | in_mw_loss: 0Epoch 2:  77%|▊| 10/13 [00:02<00:00,  4.46batch/s, Loss: 0.9326 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:02<00:00,  4.72batch/s, Loss: 0.9326 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:02<00:00,  4.72batch/s, Loss: 1.0522 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  4.41batch/s, Loss: 1.0522 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  4.41batch/s, Loss: 1.0255 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:02<00:00,  4.71batch/s, Loss: 1.0255 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:02<00:00,  4.55batch/s, Loss: 1.0255 | in_mw_loss: 1
Epoch [  2/  2], Loss: 1.0603, in_mw_loss: 1.0603, out_mw_loss: 1.0603, learning_rate: 3.0000e-04
data_shape torch.Size([320, 1, 96, 96, 96])
predict:   0%|                                         | 0/160 [00:00<?, ?it/s]predict:   1%|▏                                | 1/160 [00:00<00:24,  6.51it/s]predict:   3%|█                                | 5/160 [00:00<00:07, 21.88it/s]predict:   6%|█▊                               | 9/160 [00:00<00:05, 28.33it/s]predict:   8%|██▌                             | 13/160 [00:00<00:04, 31.70it/s]predict:  11%|███▍                            | 17/160 [00:00<00:04, 33.74it/s]predict:  13%|████▏                           | 21/160 [00:00<00:03, 34.91it/s]predict:  16%|█████                           | 25/160 [00:00<00:03, 35.62it/s]predict:  18%|█████▊                          | 29/160 [00:00<00:03, 36.01it/s]predict:  21%|██████▌                         | 33/160 [00:01<00:03, 36.43it/s]predict:  23%|███████▍                        | 37/160 [00:01<00:03, 36.65it/s]predict:  26%|████████▏                       | 41/160 [00:01<00:03, 36.90it/s]predict:  28%|█████████                       | 45/160 [00:01<00:03, 37.51it/s]predict:  31%|█████████▊                      | 49/160 [00:01<00:02, 37.71it/s]predict:  33%|██████████▌                     | 53/160 [00:01<00:02, 37.31it/s]predict:  36%|███████████▍                    | 57/160 [00:01<00:02, 37.52it/s]predict:  38%|████████████▏                   | 61/160 [00:01<00:02, 37.77it/s]predict:  41%|█████████████                   | 65/160 [00:01<00:02, 37.80it/s]predict:  43%|█████████████▊                  | 69/160 [00:01<00:02, 38.11it/s]predict:  46%|██████████████▌                 | 73/160 [00:02<00:02, 38.15it/s]predict:  48%|███████████████▍                | 77/160 [00:02<00:02, 37.88it/s]predict:  51%|████████████████▏               | 81/160 [00:02<00:02, 37.73it/s]predict:  53%|█████████████████               | 85/160 [00:02<00:02, 37.48it/s]predict:  56%|█████████████████▊              | 89/160 [00:02<00:01, 37.45it/s]predict:  58%|██████████████████▌             | 93/160 [00:02<00:01, 37.37it/s]predict:  61%|███████████████████▍            | 97/160 [00:02<00:01, 37.61it/s]predict:  63%|███████████████████▌           | 101/160 [00:02<00:01, 37.60it/s]predict:  66%|████████████████████▎          | 105/160 [00:02<00:01, 37.79it/s]predict:  68%|█████████████████████          | 109/160 [00:03<00:01, 37.68it/s]predict:  71%|█████████████████████▉         | 113/160 [00:03<00:01, 37.42it/s]predict:  73%|██████████████████████▋        | 117/160 [00:03<00:01, 37.34it/s]predict:  76%|███████████████████████▍       | 121/160 [00:03<00:01, 37.38it/s]predict:  78%|████████████████████████▏      | 125/160 [00:03<00:00, 37.51it/s]predict:  81%|████████████████████████▉      | 129/160 [00:03<00:00, 37.63it/s]predict:  83%|█████████████████████████▊     | 133/160 [00:03<00:00, 37.62it/s]predict:  86%|██████████████████████████▌    | 137/160 [00:03<00:00, 37.44it/s]predict:  88%|███████████████████████████▎   | 141/160 [00:03<00:00, 37.25it/s]predict:  91%|████████████████████████████   | 145/160 [00:03<00:00, 36.85it/s]predict:  93%|████████████████████████████▊  | 149/160 [00:04<00:00, 36.95it/s]predict:  96%|█████████████████████████████▋ | 153/160 [00:04<00:00, 37.05it/s]predict:  98%|██████████████████████████████▍| 157/160 [00:04<00:00, 37.15it/s]predict: 100%|███████████████████████████████| 160/160 [00:04<00:00, 36.39it/s]
size restored (330, 522, 522)
Saved slices and power spectrum for epoch 2 to 'denoise'
training tomograms for 2 epochs, remaining epochs 4
Epoch 1:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 1:   0%| | 0/13 [00:19<?, ?batch/s, Loss: 1.7161 | in_mw_loss: 1.7161 | oEpoch 1:   8%| | 1/13 [00:19<03:51, 19.29s/batch, Loss: 1.7161 | in_mw_loss: 1.Epoch 1:   8%| | 1/13 [00:19<03:51, 19.29s/batch, Loss: 1.5927 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:28,  8.08s/batch, Loss: 1.5927 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:28,  8.08s/batch, Loss: 1.3913 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.47s/batch, Loss: 1.3913 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.47s/batch, Loss: 1.3110 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:19<00:25,  2.81s/batch, Loss: 1.3110 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.81s/batch, Loss: 1.1394 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.86s/batch, Loss: 1.1394 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.86s/batch, Loss: 1.2695 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.31s/batch, Loss: 1.2695 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.31s/batch, Loss: 1.1306 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.07batch/s, Loss: 1.1306 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.07batch/s, Loss: 1.1547 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.43batch/s, Loss: 1.1547 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.43batch/s, Loss: 1.1501 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:20<00:02,  1.78batch/s, Loss: 1.1501 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:21<00:02,  1.78batch/s, Loss: 1.1432 | in_mw_loss: 1.Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.34batch/s, Loss: 1.1432 | in_mw_loss: 1Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.34batch/s, Loss: 1.1407 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.98batch/s, Loss: 1.1407 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.98batch/s, Loss: 1.1564 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.52batch/s, Loss: 1.1564 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.52batch/s, Loss: 1.1155 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  4.03batch/s, Loss: 1.1155 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  1.66s/batch, Loss: 1.1155 | in_mw_loss: 1
Epoch [  1/  2], Loss: 1.2668, in_mw_loss: 1.2668, out_mw_loss: 1.2668, learning_rate: 3.0000e-04
Epoch 2:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 2:   0%| | 0/13 [00:00<?, ?batch/s, Loss: 1.1075 | in_mw_loss: 1.1075 | oEpoch 2:   8%| | 1/13 [00:00<00:02,  4.25batch/s, Loss: 1.1075 | in_mw_loss: 1.Epoch 2:   8%| | 1/13 [00:00<00:02,  4.25batch/s, Loss: 1.1159 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:01,  5.81batch/s, Loss: 1.1159 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:01,  5.81batch/s, Loss: 1.0803 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  6.51batch/s, Loss: 1.0803 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  6.51batch/s, Loss: 1.1428 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  6.87batch/s, Loss: 1.1428 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  6.87batch/s, Loss: 1.0596 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:00<00:01,  7.13batch/s, Loss: 1.0596 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:00<00:01,  7.13batch/s, Loss: 1.0473 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:00<00:00,  7.02batch/s, Loss: 1.0473 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:00,  7.02batch/s, Loss: 1.0248 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:00,  7.02batch/s, Loss: 1.0248 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:00,  7.02batch/s, Loss: 1.0403 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  7.18batch/s, Loss: 1.0403 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:00,  7.18batch/s, Loss: 1.0679 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  7.36batch/s, Loss: 1.0679 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  7.36batch/s, Loss: 0.9835 | in_mw_loss: 0.Epoch 2:  77%|▊| 10/13 [00:01<00:00,  7.42batch/s, Loss: 0.9835 | in_mw_loss: 0Epoch 2:  77%|▊| 10/13 [00:01<00:00,  7.42batch/s, Loss: 0.9323 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:01<00:00,  7.51batch/s, Loss: 0.9323 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:01<00:00,  7.51batch/s, Loss: 1.0543 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:01<00:00,  6.90batch/s, Loss: 1.0543 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:01<00:00,  6.90batch/s, Loss: 1.0276 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:01<00:00,  6.66batch/s, Loss: 1.0276 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:01<00:00,  6.82batch/s, Loss: 1.0276 | in_mw_loss: 1
Epoch [  2/  2], Loss: 1.0603, in_mw_loss: 1.0603, out_mw_loss: 1.0603, learning_rate: 3.0000e-04
data_shape torch.Size([320, 1, 96, 96, 96])
predict:   0%|                                         | 0/160 [00:00<?, ?it/s]predict:   1%|▏                                | 1/160 [00:00<00:25,  6.35it/s]predict:   3%|█                                | 5/160 [00:00<00:07, 21.58it/s]predict:   6%|█▊                               | 9/160 [00:00<00:05, 28.13it/s]predict:   8%|██▌                             | 13/160 [00:00<00:04, 31.54it/s]predict:  11%|███▍                            | 17/160 [00:00<00:04, 33.75it/s]predict:  13%|████▏                           | 21/160 [00:00<00:03, 35.23it/s]predict:  16%|█████                           | 25/160 [00:00<00:03, 35.99it/s]predict:  18%|█████▊                          | 29/160 [00:00<00:03, 36.74it/s]predict:  21%|██████▌                         | 33/160 [00:01<00:03, 37.33it/s]predict:  23%|███████▍                        | 37/160 [00:01<00:03, 37.66it/s]predict:  26%|████████▏                       | 41/160 [00:01<00:03, 37.62it/s]predict:  28%|█████████                       | 45/160 [00:01<00:03, 37.68it/s]predict:  31%|█████████▊                      | 49/160 [00:01<00:02, 37.86it/s]predict:  33%|██████████▌                     | 53/160 [00:01<00:02, 37.76it/s]predict:  36%|███████████▍                    | 57/160 [00:01<00:02, 37.95it/s]predict:  38%|████████████▏                   | 61/160 [00:01<00:02, 38.17it/s]predict:  41%|█████████████                   | 65/160 [00:01<00:02, 37.98it/s]predict:  43%|█████████████▊                  | 69/160 [00:01<00:02, 37.64it/s]predict:  46%|██████████████▌                 | 73/160 [00:02<00:02, 37.96it/s]predict:  48%|███████████████▍                | 77/160 [00:02<00:02, 37.88it/s]predict:  51%|████████████████▏               | 81/160 [00:02<00:02, 37.98it/s]predict:  53%|█████████████████               | 85/160 [00:02<00:01, 38.19it/s]predict:  56%|█████████████████▊              | 89/160 [00:02<00:01, 37.90it/s]predict:  58%|██████████████████▌             | 93/160 [00:02<00:01, 37.91it/s]predict:  61%|███████████████████▍            | 97/160 [00:02<00:01, 37.85it/s]predict:  63%|███████████████████▌           | 101/160 [00:02<00:01, 38.13it/s]predict:  66%|████████████████████▎          | 105/160 [00:02<00:01, 38.29it/s]predict:  68%|█████████████████████          | 109/160 [00:03<00:01, 37.90it/s]predict:  71%|█████████████████████▉         | 113/160 [00:03<00:01, 37.99it/s]predict:  73%|██████████████████████▋        | 117/160 [00:03<00:01, 38.22it/s]predict:  76%|███████████████████████▍       | 121/160 [00:03<00:01, 38.41it/s]predict:  78%|████████████████████████▏      | 125/160 [00:03<00:00, 38.45it/s]predict:  81%|████████████████████████▉      | 129/160 [00:03<00:00, 38.02it/s]predict:  83%|█████████████████████████▊     | 133/160 [00:03<00:00, 37.92it/s]predict:  86%|██████████████████████████▌    | 137/160 [00:03<00:00, 38.16it/s]predict:  88%|███████████████████████████▎   | 141/160 [00:03<00:00, 38.26it/s]predict:  91%|████████████████████████████   | 145/160 [00:03<00:00, 38.20it/s]predict:  93%|████████████████████████████▊  | 149/160 [00:04<00:00, 38.26it/s]predict:  96%|█████████████████████████████▋ | 153/160 [00:04<00:00, 38.30it/s]predict:  98%|██████████████████████████████▍| 157/160 [00:04<00:00, 38.42it/s]predict: 100%|███████████████████████████████| 160/160 [00:04<00:00, 36.91it/s]
size restored (330, 522, 522)
Saved slices and power spectrum for epoch 4 to 'denoise'
training tomograms for 2 epochs, remaining epochs 2
Epoch 1:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 1:   0%| | 0/13 [00:19<?, ?batch/s, Loss: 1.7159 | in_mw_loss: 1.7159 | oEpoch 1:   8%| | 1/13 [00:19<03:50, 19.21s/batch, Loss: 1.7159 | in_mw_loss: 1.Epoch 1:   8%| | 1/13 [00:19<03:50, 19.21s/batch, Loss: 1.5927 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:27,  7.99s/batch, Loss: 1.5927 | in_mw_loss: 1.Epoch 1:  15%|▏| 2/13 [00:19<01:27,  7.99s/batch, Loss: 1.3917 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.48s/batch, Loss: 1.3917 | in_mw_loss: 1.Epoch 1:  23%|▏| 3/13 [00:19<00:44,  4.48s/batch, Loss: 1.3114 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:19<00:25,  2.79s/batch, Loss: 1.3114 | in_mw_loss: 1.Epoch 1:  31%|▎| 4/13 [00:20<00:25,  2.79s/batch, Loss: 1.1417 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.84s/batch, Loss: 1.1417 | in_mw_loss: 1.Epoch 1:  38%|▍| 5/13 [00:20<00:14,  1.84s/batch, Loss: 1.2696 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.31s/batch, Loss: 1.2696 | in_mw_loss: 1.Epoch 1:  46%|▍| 6/13 [00:20<00:09,  1.31s/batch, Loss: 1.1315 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.04batch/s, Loss: 1.1315 | in_mw_loss: 1.Epoch 1:  54%|▌| 7/13 [00:20<00:05,  1.04batch/s, Loss: 1.1566 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.43batch/s, Loss: 1.1566 | in_mw_loss: 1.Epoch 1:  62%|▌| 8/13 [00:20<00:03,  1.43batch/s, Loss: 1.1489 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:20<00:02,  1.91batch/s, Loss: 1.1489 | in_mw_loss: 1.Epoch 1:  69%|▋| 9/13 [00:20<00:02,  1.91batch/s, Loss: 1.1422 | in_mw_loss: 1.Epoch 1:  77%|▊| 10/13 [00:20<00:01,  2.46batch/s, Loss: 1.1422 | in_mw_loss: 1Epoch 1:  77%|▊| 10/13 [00:21<00:01,  2.46batch/s, Loss: 1.1441 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.69batch/s, Loss: 1.1441 | in_mw_loss: 1Epoch 1:  85%|▊| 11/13 [00:21<00:00,  2.69batch/s, Loss: 1.1565 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.12batch/s, Loss: 1.1565 | in_mw_loss: 1Epoch 1:  92%|▉| 12/13 [00:21<00:00,  3.12batch/s, Loss: 1.1183 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  3.82batch/s, Loss: 1.1183 | in_mw_loss: 1Epoch 1: 100%|█| 13/13 [00:21<00:00,  1.66s/batch, Loss: 1.1183 | in_mw_loss: 1
Epoch [  1/  2], Loss: 1.2673, in_mw_loss: 1.2673, out_mw_loss: 1.2673, learning_rate: 3.0000e-04
Epoch 2:   0%|                                       | 0/13 [00:00<?, ?batch/s]Epoch 2:   0%| | 0/13 [00:00<?, ?batch/s, Loss: 1.1075 | in_mw_loss: 1.1075 | oEpoch 2:   8%| | 1/13 [00:00<00:03,  3.95batch/s, Loss: 1.1075 | in_mw_loss: 1.Epoch 2:   8%| | 1/13 [00:00<00:03,  3.95batch/s, Loss: 1.1159 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  5.04batch/s, Loss: 1.1159 | in_mw_loss: 1.Epoch 2:  15%|▏| 2/13 [00:00<00:02,  5.04batch/s, Loss: 1.0795 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.20batch/s, Loss: 1.0795 | in_mw_loss: 1.Epoch 2:  23%|▏| 3/13 [00:00<00:01,  5.20batch/s, Loss: 1.1438 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:00<00:01,  4.59batch/s, Loss: 1.1438 | in_mw_loss: 1.Epoch 2:  31%|▎| 4/13 [00:01<00:01,  4.59batch/s, Loss: 1.0603 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.60batch/s, Loss: 1.0603 | in_mw_loss: 1.Epoch 2:  38%|▍| 5/13 [00:01<00:01,  4.60batch/s, Loss: 1.0465 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.66batch/s, Loss: 1.0465 | in_mw_loss: 1.Epoch 2:  46%|▍| 6/13 [00:01<00:01,  4.66batch/s, Loss: 1.0253 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  4.40batch/s, Loss: 1.0253 | in_mw_loss: 1.Epoch 2:  54%|▌| 7/13 [00:01<00:01,  4.40batch/s, Loss: 1.0425 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:01,  4.87batch/s, Loss: 1.0425 | in_mw_loss: 1.Epoch 2:  62%|▌| 8/13 [00:01<00:01,  4.87batch/s, Loss: 1.0648 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:01<00:00,  4.95batch/s, Loss: 1.0648 | in_mw_loss: 1.Epoch 2:  69%|▋| 9/13 [00:02<00:00,  4.95batch/s, Loss: 0.9840 | in_mw_loss: 0.Epoch 2:  77%|▊| 10/13 [00:02<00:00,  4.50batch/s, Loss: 0.9840 | in_mw_loss: 0Epoch 2:  77%|▊| 10/13 [00:02<00:00,  4.50batch/s, Loss: 0.9353 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:02<00:00,  4.41batch/s, Loss: 0.9353 | in_mw_loss: 0Epoch 2:  85%|▊| 11/13 [00:02<00:00,  4.41batch/s, Loss: 1.0551 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  4.61batch/s, Loss: 1.0551 | in_mw_loss: 1Epoch 2:  92%|▉| 12/13 [00:02<00:00,  4.61batch/s, Loss: 1.0277 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:02<00:00,  4.84batch/s, Loss: 1.0277 | in_mw_loss: 1Epoch 2: 100%|█| 13/13 [00:02<00:00,  4.69batch/s, Loss: 1.0277 | in_mw_loss: 1
Epoch [  2/  2], Loss: 1.0605, in_mw_loss: 1.0605, out_mw_loss: 1.0605, learning_rate: 3.0000e-04
data_shape torch.Size([320, 1, 96, 96, 96])
W0718 20:13:49.709000 1543617 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 1552271 via signal SIGTERM
Traceback (most recent call last):
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 1080, in <module>
    exit(main())
         ~~~~^^
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 1076, in main
    fire.Fire(ISONET)
    ~~~~~~~~~^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ~~~~~~~~~~~~~~~~~~~^
        component,
        ^^^^^^^^^^
    ...<2 lines>...
        treatment='class' if is_class else 'routine',
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        target=component.__name__)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/cii/software/IsoNet2/IsoNet/bin/isonet.py", line 604, in denoise
    network.train(training_params) #train based on init model and save new one as model_iter{num_iter}.h5
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 265, in train
    tmp_out = self.predict_map(tomo_vol,output_dir=f"{training_params['output_dir']}", F_mask=F_mask).astype(np.float32) * -1
              ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 323, in predict_map
    outData = self.predict(data, tmp_data_path=tmp_data_path, F_mask=F_mask)
  File "/home/cii/software/IsoNet2/IsoNet/models/network.py", line 302, in predict
    mp.spawn(ddp_predict, args=(self.world_size, self.port_number, self.model, data, tmp_data_path,\
    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                 F_mask), nprocs=self.world_size)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
              ~~~~~~~~~~~~^^
  File "/home/cii/anaconda3/envs/torch_cuda12.6_glados_py3.13/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 196, in join
    raise ProcessExitedException(
    ...<5 lines>...
    )
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGSEGV
Process exited with code 1
